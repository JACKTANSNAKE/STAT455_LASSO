# How Ridge Works
Recall from Linear Algebra that if we want to solve for $\boldsymbol{A}x = b$, we can do:
$$
\boldsymbol{A}^T\boldsymbol{A}x = \boldsymbol{A}^Ty
$$
We then take the inverse of $\boldsymbol{A}^T\boldsymbol{A}$ from the LHS, and divide both sides by the inverse:
$$
x = (\boldsymbol{A}^T\boldsymbol{A})^{-1}\boldsymbol{A}^Ty
$$
Then we have come up with a least squares solution for x in $Ax = b$.
We will use a very similar approach to show how the shrinkage work for ridge regression.

### Univariate example
Let's consider a very simple model: $y = \beta x + \epsilon$, with an L2(ridge regression) penalty on $\hat\beta$ and a least-squares loss function on $\hat{\epsilon}$. We can then expand the expression for sum of squared residuals to be minimized as:
$$
\hat\epsilon = arg\min_\beta\{\sum_{i = 1}^{N}(y_i - x_i\beta)^2 + \lambda\sum_{j = 1}^{p}\beta_j^2\}
$$
Which if we transform into a Matrix form, gives:
$$
\hat{\epsilon} = arg\min_\beta\{(\vec{y} - \vec{x}\hat\beta)^{T}(\vec{y} - \vec{x}\hat\beta) + \lambda\hat{\beta}^2\}
$$
Which we can further expand into:
$$
\hat\epsilon = arg\min_\beta\{\vec{y}^{T}\vec{y} - 2\vec{y}^T\vec{x}\hat{\beta} + \hat{\beta}\vec{x}^T\vec{x}\hat{\beta} + \lambda\hat{\beta}^2\}
$$
Now if we take the derivative w.r.t $\hat\beta$ and set equal to 0, we can calculate the the estimator for ridge regression coefficients $\hat\beta$:
$$
-2\vec{y}^T\vec{x} + 2\vec{x}^T\vec{x}\hat{\beta} + 2\lambda\hat{\beta} =_{set} 0
$$
Where we can obtain:
$$
\hat{\beta} = \vec{y}^T\vec{x}{(\vec{x}^T\vec{x} + \lambda)}^{-1}
$$

### Higher dimension ridge regression
Note that here we are using an univariate example(1-dimensional y and 1-dimensional x), in a more complicated case, we calculate the sum of squared residual in the same manner:
$$
RSS(\lambda) = (\boldsymbol{y} - \boldsymbol{X}\beta)^T(\boldsymbol{y} - \boldsymbol{X}\beta) + \lambda\beta^T\beta
$$
Where we can get:
$$
\hat{\beta} = \boldsymbol{X}^T\boldsymbol{y}{(\boldsymbol{X}^T\boldsymbol{X} + \lambda\boldsymbol{I})}^{-1}
$$
Which is very similar to the result from the univariate example.
Now, as we have mentioned above, $\lambda$ is the tuning parameter, which shrinks the coefficients more if it gets larger, thus shrinkage.