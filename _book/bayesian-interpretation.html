<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Bayesian Interpretation | Shrinkage Methods</title>
  <meta name="description" content="Chapter 7 Bayesian Interpretation | Shrinkage Methods" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Bayesian Interpretation | Shrinkage Methods" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Bayesian Interpretation | Shrinkage Methods" />
  
  
  



<meta name="date" content="2021-03-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="applications-of-ridge-and-lasso.html"/>

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Motivation</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#the-ordinary-least-squares-model"><i class="fa fa-check"></i><b>1.1</b> The ordinary least squares model</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#several-concepts-for-better-linear-regression-model"><i class="fa fa-check"></i><b>1.2</b> Several concepts for better linear regression model</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#shrinkage"><i class="fa fa-check"></i><b>1.3</b> Shrinkage:</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#literature-review"><i class="fa fa-check"></i><b>1.4</b> Literature Review</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#outline"><i class="fa fa-check"></i><b>1.5</b> Outline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>2</b> Ridge Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="ridge-regression.html"><a href="ridge-regression.html#definition"><i class="fa fa-check"></i><b>2.1</b> Definition</a></li>
<li class="chapter" data-level="2.2" data-path="ridge-regression.html"><a href="ridge-regression.html#ridge-regression-1"><i class="fa fa-check"></i><b>2.2</b> Ridge Regression:</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="how-ridge-works.html"><a href="how-ridge-works.html"><i class="fa fa-check"></i><b>3</b> How Ridge Works</a><ul>
<li class="chapter" data-level="3.0.1" data-path="how-ridge-works.html"><a href="how-ridge-works.html#univariate-example"><i class="fa fa-check"></i><b>3.0.1</b> Univariate example</a></li>
<li class="chapter" data-level="3.0.2" data-path="how-ridge-works.html"><a href="how-ridge-works.html#higher-dimension-ridge-regression"><i class="fa fa-check"></i><b>3.0.2</b> Higher dimension ridge regression</a></li>
<li class="chapter" data-level="3.0.3" data-path="how-ridge-works.html"><a href="how-ridge-works.html#the-tuning-parameter"><i class="fa fa-check"></i><b>3.0.3</b> The tuning parameter</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>4</b> Lasso</a><ul>
<li class="chapter" data-level="4.0.1" data-path="lasso.html"><a href="lasso.html#standardizing-the-dataset"><i class="fa fa-check"></i><b>4.0.1</b> Standardizing the dataset</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="how-lasso-performs-variable-selection.html"><a href="how-lasso-performs-variable-selection.html"><i class="fa fa-check"></i><b>5</b> How Lasso performs variable selection</a><ul>
<li class="chapter" data-level="5.0.1" data-path="how-lasso-performs-variable-selection.html"><a href="how-lasso-performs-variable-selection.html#univariate-example-1"><i class="fa fa-check"></i><b>5.0.1</b> univariate example</a></li>
<li class="chapter" data-level="5.0.2" data-path="how-lasso-performs-variable-selection.html"><a href="how-lasso-performs-variable-selection.html#shrinkage-explanation-from-geometric-perspective"><i class="fa fa-check"></i><b>5.0.2</b> Shrinkage explanation from geometric perspective</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="applications-of-ridge-and-lasso.html"><a href="applications-of-ridge-and-lasso.html"><i class="fa fa-check"></i><b>6</b> Applications of Ridge and Lasso</a><ul>
<li class="chapter" data-level="6.1" data-path="applications-of-ridge-and-lasso.html"><a href="applications-of-ridge-and-lasso.html#cross-validation"><i class="fa fa-check"></i><b>6.1</b> Cross Validation</a></li>
<li class="chapter" data-level="6.2" data-path="applications-of-ridge-and-lasso.html"><a href="applications-of-ridge-and-lasso.html#dataset"><i class="fa fa-check"></i><b>6.2</b> Dataset</a></li>
<li class="chapter" data-level="6.3" data-path="applications-of-ridge-and-lasso.html"><a href="applications-of-ridge-and-lasso.html#ols"><i class="fa fa-check"></i><b>6.3</b> OLS</a></li>
<li class="chapter" data-level="6.4" data-path="applications-of-ridge-and-lasso.html"><a href="applications-of-ridge-and-lasso.html#ridge"><i class="fa fa-check"></i><b>6.4</b> RIDGE</a></li>
<li class="chapter" data-level="6.5" data-path="applications-of-ridge-and-lasso.html"><a href="applications-of-ridge-and-lasso.html#lasso-1"><i class="fa fa-check"></i><b>6.5</b> LASSO</a></li>
<li class="chapter" data-level="6.6" data-path="applications-of-ridge-and-lasso.html"><a href="applications-of-ridge-and-lasso.html#test-data"><i class="fa fa-check"></i><b>6.6</b> Test data</a></li>
<li class="chapter" data-level="6.7" data-path="applications-of-ridge-and-lasso.html"><a href="applications-of-ridge-and-lasso.html#summary"><i class="fa fa-check"></i><b>6.7</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian-interpretation.html"><a href="bayesian-interpretation.html"><i class="fa fa-check"></i><b>7</b> Bayesian Interpretation</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian-interpretation.html"><a href="bayesian-interpretation.html#bayesian-interpretation-of-lasso"><i class="fa fa-check"></i><b>7.1</b> Bayesian Interpretation of Lasso</a></li>
<li class="chapter" data-level="7.2" data-path="bayesian-interpretation.html"><a href="bayesian-interpretation.html#posterior"><i class="fa fa-check"></i><b>7.2</b> Posterior</a></li>
<li class="chapter" data-level="7.3" data-path="bayesian-interpretation.html"><a href="bayesian-interpretation.html#posterior-mode"><i class="fa fa-check"></i><b>7.3</b> Posterior Mode</a></li>
<li class="chapter" data-level="7.4" data-path="bayesian-interpretation.html"><a href="bayesian-interpretation.html#bayesian-interpretation-of-ridge"><i class="fa fa-check"></i><b>7.4</b> Bayesian Interpretation of Ridge</a></li>
<li class="chapter" data-level="7.5" data-path="bayesian-interpretation.html"><a href="bayesian-interpretation.html#posterior-1"><i class="fa fa-check"></i><b>7.5</b> Posterior</a></li>
<li class="chapter" data-level="7.6" data-path="bayesian-interpretation.html"><a href="bayesian-interpretation.html#posterior-mode-1"><i class="fa fa-check"></i><b>7.6</b> Posterior Mode</a></li>
<li class="chapter" data-level="7.7" data-path="bayesian-interpretation.html"><a href="bayesian-interpretation.html#limitations-of-ridge-and-lasso"><i class="fa fa-check"></i><b>7.7</b> Limitations of Ridge and Lasso</a></li>
<li class="chapter" data-level="7.8" data-path="bayesian-interpretation.html"><a href="bayesian-interpretation.html#reference"><i class="fa fa-check"></i><b>7.8</b> Reference</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Shrinkage Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesian-interpretation" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Bayesian Interpretation</h1>
<div id="bayesian-interpretation-of-lasso" class="section level2">
<h2><span class="header-section-number">7.1</span> Bayesian Interpretation of Lasso</h2>
<p>We can show that <span class="math inline">\(\hat{\boldsymbol\beta}_{LASSO}\)</span> has a Bayesian interpretation. In particular, we can show that is a Bayes estimator for <span class="math inline">\(\boldsymbol\beta\)</span> assuming a multivariate Normal likelihood for <span class="math inline">\(\mathbf{y}\)</span>:
<span class="math display">\[f(y\mid \boldsymbol\beta, \sigma^2)\sim N(\mathbf{X}\boldsymbol\beta, \sigma^2\mathbf{I}),\]</span>
and an independent double exponential (aka Laplace) prior for <span class="math inline">\(\boldsymbol\beta\)</span>:
<span class="math display">\[f(\beta_j) = \left(\frac{\lambda}{2} \right)\exp(-\lambda |\beta_j|)\]</span></p>
<p><img src="Shrinkage-Methods_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>The above is an example of what the distribution of the double exponential (aka Laplace) prior looks like. From the grapg, we can see that as it goes towards the two sides, the density gradually goes to exactly 0. This explains why it is possible for Lasso coefficients to shrink towards 0 and ultimately be exactly 0.</p>
</div>
<div id="posterior" class="section level2">
<h2><span class="header-section-number">7.2</span> Posterior</h2>
<p>The posterior distribution for <span class="math inline">\(\boldsymbol\beta\)</span> (assuming <span class="math inline">\(\sigma^2 = 1\)</span> for simplicity):</p>
<p><span class="math display">\[\begin{aligned}
g(\boldsymbol\beta \mid y)&amp;\propto f(y \mid \boldsymbol\beta, \sigma^2) f(\boldsymbol\beta)\\
&amp;= f(y \mid \boldsymbol\beta) \prod_{i=1}^p f(\beta_j)\text{, by independence assumption}\\
&amp;=(2\pi)^{-n/2}\exp\left(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta) \right) \left(\frac{\lambda}{2}\right)^p \exp\left(-\lambda\sum_{j=1}^p |\beta_j|)\right)\\
&amp;\propto\exp\left(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta) \right) \exp\left(-\lambda\sum_{j=1}^p |\beta_j|)\right)\\
&amp;=\exp\left(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)-\lambda\sum_{j=1}^p |\beta_j|)\right)
\end{aligned}\]</span></p>
</div>
<div id="posterior-mode" class="section level2">
<h2><span class="header-section-number">7.3</span> Posterior Mode</h2>
<p>Maximizing the posterior distribution, or minimizing the <span class="math inline">\(-\log\)</span> posterior, leads to <span class="math inline">\(\hat\beta_{LASSO}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}\\
&amp;\arg\max\text{ }\exp\left(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)-\lambda\sum_{j=1}^p |\beta_j|)\right)\\
=\text{ }&amp;\arg\max\text{ }-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)-\lambda\sum_{j=1}^p |\beta_j|\\
=\text{ }&amp;\arg\min\text{ }\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)+\lambda\sum_{j=1}^p |\beta_j|\\
=\text{ }&amp;\arg\min\text{ -log posterior}\\
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\arg\min\text{ -log posterior}
&amp;=\arg\min-\log(\exp\left(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)-\lambda\sum_{j=1}^p |\beta_j|)\right))\\
&amp;=\arg\min-\left(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)-\lambda\sum_{j=1}^p |\beta_j|)\right)\\
&amp;=\arg\min\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)+\lambda\sum_{j=1}^p |\beta_j|\\
&amp;=\hat\beta_{LASSO}\text{, by definition}
\end{aligned}\]</span></p>
</div>
<div id="bayesian-interpretation-of-ridge" class="section level2">
<h2><span class="header-section-number">7.4</span> Bayesian Interpretation of Ridge</h2>
<p>In a similar way, we can show that <span class="math inline">\(\hat{\boldsymbol\beta}_{RIDGE}\)</span> has a Bayesian interpretation by using a different prior. In particular, we can show that is a Bayes estimator for <span class="math inline">\(\boldsymbol\beta\)</span> assuming a multivariate Normal likelihood for <span class="math inline">\(\mathbf{y}\)</span>:
<span class="math display">\[f(y\mid \boldsymbol\beta, \sigma^2)\sim N(\mathbf{X}\boldsymbol\beta, \sigma^2\mathbf{I}),\]</span>
and an independent normal prior for <span class="math inline">\(\boldsymbol\beta\)</span>:
<span class="math display">\[f(\beta_j) = \left(\frac{\lambda}{\sigma^2\sqrt{2\pi}} \right)\exp(-\frac{1}{2}(\frac{\lambda\beta_j}{\sigma^2})^2)\]</span></p>
<p><img src="Shrinkage-Methods_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>The above is an example of what the distribution of the normal prior looks like. From the grapg, we can see that as it goes towards the two sides, the density gradually shrinks towards 0. This explains why it is possible for Ridge coefficients to shrink towards 0 and cannot goes to exactly 0.</p>
</div>
<div id="posterior-1" class="section level2">
<h2><span class="header-section-number">7.5</span> Posterior</h2>
<p>The posterior distribution for <span class="math inline">\(\boldsymbol\beta\)</span> (assuming <span class="math inline">\(\sigma^2 = 1\)</span> for simplicity):</p>
<p><span class="math display">\[\begin{aligned}
g(\boldsymbol\beta \mid y)&amp;\propto f(y \mid \boldsymbol\beta, \sigma^2) f(\boldsymbol\beta)\\
&amp;= f(y \mid \boldsymbol\beta) \prod_{i=1}^p f(\beta_j)\text{, by independence assumption}\\
&amp;=(2\pi)^{-n/2}\exp\left(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta) \right) \left(\frac{\lambda}{\sqrt{2\pi}} \right)^p\exp(-\frac{\lambda^2}{2}\sum_{j=1}^p\beta_j^2)\\
&amp;\propto\exp\left(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta) \right) \exp\left(-\lambda^*\sum_{j=1}^p\beta_j^2\right)\\
&amp;=\exp\left(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)-\lambda^*\sum_{j=1}^p\beta_j^2\right)
\end{aligned}\]</span></p>
</div>
<div id="posterior-mode-1" class="section level2">
<h2><span class="header-section-number">7.6</span> Posterior Mode</h2>
<p>Maximizing the posterior distribution, or minimizing the <span class="math inline">\(-\log\)</span> posterior, leads to <span class="math inline">\(\hat\beta_{RIDGE}\)</span>.</p>
<p><span class="math display">\[\begin{aligned}\\
&amp;\arg\max\text{ }\exp\left(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)-\lambda^*\sum_{j=1}^p\beta_j^2\right)\\
=\text{ }&amp;\arg\max\text{ }-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)-\lambda^*\sum_{j=1}^p\beta_j^2\\
=\text{ }&amp;\arg\min\text{ }\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)+\lambda^*\sum_{j=1}^p\beta_j^2\\
=\text{ }&amp;\arg\min\text{ -log posterior}\\
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\arg\min\text{ -log posterior}
&amp;=\arg\min-\log(\exp\left(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)-\lambda^*\sum_{j=1}^p\beta_j^2\right))\\
&amp;=\arg\min-\left(-\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)-\lambda^*\sum_{j=1}^p\beta_j^2\right)\\
&amp;=\arg\min\frac{1}{2}(\mathbf{y} - \mathbf{X}\boldsymbol\beta)^\top (\mathbf{y} - \mathbf{X}\boldsymbol\beta)+\lambda^*\sum_{j=1}^p\beta_j^2\\
&amp;=\hat\beta_{RIDGE}\text{, by definition}
\end{aligned}\]</span></p>

</div>
<div id="limitations-of-ridge-and-lasso" class="section level2">
<h2><span class="header-section-number">7.7</span> Limitations of Ridge and Lasso</h2>
<ul>
<li><p>According to our empirical results, we find that both ridge and lasso are better at making predictions than the OLS model.</p></li>
<li><p>The coefficients in Lasso can reach zero while those in ridge can never be zero. If we want to perform variable selection, lasso might be a better choice.</p></li>
<li><p>One important drawback of Lasso and ridge is that it’s very hard/impossible to do proper inference (i.e., get confidence intervals and p-values)—you really just get estimates and that’s it.</p></li>
<li><p>The other thing is that we usually cannot give a good explanation for the coefficients and the entire model, because the coefficients are shrunk. Some might say we could use Lasso simply as a variable selection tool and we can use the Lasso result to generate an OLS which is easier to explain. But note that there is possibility that Lasso would give us wrong information, based on assumptions we have made about the data.</p></li>
<li><p>In real examples, when dealing with categorical variables(those with more than 10 sub-categories), Lasso often does weird thing as it treats each of the sub-category as an individual variable, that is, it selects some of the sub-categories while think others to be irrelevant. This also makes it hard to interpret.</p></li>
</ul>
</div>
<div id="reference" class="section level2">
<h2><span class="header-section-number">7.8</span> Reference</h2>
<ul>
<li><p>Hastie, T., Friedman, J., &amp; Tisbshirani, R. (2017). The Elements of statistical learning: data mining, inference, and prediction. New York: Springer.</p></li>
<li><p>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2017). An introduction to statistical learning: with applications in R. New York: Springer.</p></li>
<li><p>Tibshirani, R. (1996). Regression Shrinkage and Selection Via the Lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1), 267–288. doi: 10.1111/j.2517-6161.1996.tb02080.x</p></li>
<li><p>Wieringen, W. N. van. (2020, January 18). Lecture Notes on Ridge Regression. PDF.</p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="applications-of-ridge-and-lasso.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
