<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 2 Motivation | index" />
<meta property="og:type" content="book" />






<meta name="date" content="2020-04-22" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Chapter 2 Motivation | index">

<title>Chapter 2 Motivation | index</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>




<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#index"><span class="toc-section-number">1</span> index</a></li>
<li><a href="2-motivation.html#motivation"><span class="toc-section-number">2</span> Motivation</a></li>
<li class="has-sub"><a href="3-ridge-regression.html#ridge-regression"><span class="toc-section-number">3</span> Ridge Regression</a><ul>
<li><a href="3-1-definition.html#definition"><span class="toc-section-number">3.1</span> Definition</a></li>
<li><a href="3-2-ridge-regression-1.html#ridge-regression-1"><span class="toc-section-number">3.2</span> Ridge Regression:</a></li>
</ul></li>
<li class="has-sub"><a href="4-how-ridge-works.html#how-ridge-works"><span class="toc-section-number">4</span> How Ridge Works</a><ul>
<li><a href="4-how-ridge-works.html#univariate-example"><span class="toc-section-number">4.0.1</span> Univariate example</a></li>
<li><a href="4-how-ridge-works.html#higher-dimension-ridge-regression"><span class="toc-section-number">4.0.2</span> Higher dimension ridge regression</a></li>
</ul></li>
<li><a href="5-lasso.html#lasso"><span class="toc-section-number">5</span> Lasso</a></li>
<li class="has-sub"><a href="6-how-lasso-works.html#how-lasso-works"><span class="toc-section-number">6</span> How Lasso Works</a><ul>
<li><a href="6-how-lasso-works.html#univariate-example-1"><span class="toc-section-number">6.0.1</span> univariate example</a></li>
<li><a href="6-how-lasso-works.html#shrinkage-explaination-from-geometric-perspective"><span class="toc-section-number">6.0.2</span> Shrinkage explaination from geometric perspective</a></li>
</ul></li>
<li><a href="7-a-real-example.html#a-real-example"><span class="toc-section-number">7</span> A Real Example</a></li>
<li class="has-sub"><a href="8-bayesian-lasso.html#bayesian-lasso"><span class="toc-section-number">8</span> Bayesian Lasso</a><ul>
<li><a href="8-1-generalization-of-shrinkage-methods-from-bayes-perspective.html#generalization-of-shrinkage-methods-from-bayes-perspective"><span class="toc-section-number">8.1</span> Generalization of shrinkage methods from Bayes perspective</a></li>
<li class="has-sub"><a href="8-2-bayesian-interpretation.html#bayesian-interpretation"><span class="toc-section-number">8.2</span> Bayesian Interpretation</a><ul>
<li><a href="8-2-bayesian-interpretation.html#posterior"><span class="toc-section-number">8.2.1</span> Posterior</a></li>
<li><a href="8-2-bayesian-interpretation.html#posterior-mode"><span class="toc-section-number">8.2.2</span> Posterior Mode</a></li>
</ul></li>
<li><a href="8-3-maximize-the-posterior-distribution.html#maximize-the-posterior-distribution"><span class="toc-section-number">8.3</span> Maximize the posterior distribution</a></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="motivation" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> Motivation</h1>
<p>The ordinary least squared model
<span class="math display">\[Y = \beta_0+\beta_1X_1+\beta_2X_2+...+\beta_nX_n+\epsilon_i\]</span>
is the mostly commonly used method to describe the relationship between the response variable Y and a set of variables X. However, the OLS model is not perfect and it faces two major criticism. First, the OLS model usually has low prediction accuracy because it often has large variance and low bias. The second weakness is efficient interpretation. If we have a dataset with large number of predictors, we would usually choose a small subset of predictors which are strongly associated with the response variable; thus, it seems to be a loss that we still include the weak predictors in our model. However, the linear regression model is easy to regress and interpret; therefore, we would still use linear regression method but we want to modify it in a way such that we could have higher prediction accuracy and stronger interpretation.</p>
<p>There are several solutions proposed by the statisticians. The first method is subset selection which is a discrete selection process. Assuming that we have p predictors, and we would build <span class="math inline">\(2^p\)</span> models to find the most predictive models within all possible combinations. However, if we have a dataset with only 10 predictors, we would have to construct 1024 models. Therefore, the subset selection is computationally inefficient. The second method is shrinkage, including ridge and lasso regression. This approach shrinks the coefficients towards zero which is a continuous selection process. The continuous selection process is less variable than the subset selection because subset selection can yield a different model if there is a small change in the dataset. Therefore, we want to use the coefficients shrinkage method to find the best model under the concept of linear regression. In the following sections, we would discuss the definition, application, Bayesian interpretation, and limitations for both ridge and lasso regression.</p>

</div>
<p style="text-align: center;">
<a href="index.html"><button class="btn btn-default">Previous</button></a>
<a href="3-ridge-regression.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
