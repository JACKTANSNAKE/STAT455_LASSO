---
title: "LASSO"
author: "JACKTANSNAKE"
date: "2020/4/15"
output: html_document
---
# Motivation:


# Definition:
Ridge Regression:
$$
\hat{\beta^{ridge}} = arg\min_\beta\{\sum_{i = 1}^{N}(y_i - \beta_0 - \sum_{j = 1}^{p}x_{ij}\beta_j)^2 + \lambda\sum_{j = 1}^{p}\beta_j^2\} \ \ \ (1)
$$
Which is the same as:
$$
\hat{\beta^{ridge}} = arg\min_\beta\{\sum_{i = 1}^{N}(y_i - \beta_0 - \sum_{j = 1}^{p}x_{ij}\beta_j)^2\}\\
\text{subject to} \sum_{j = 1}^{p}\beta_j^2 \leq t
$$
where there is a one-to-one corresondence between the parameters $\lambda$ and $t$.
Lasso Regression:
$$
\hat{\beta^{Lasso}} = arg\min_\beta\{\sum_{i = 1}^{N}(y_i - \beta_0 - \sum_{j = 1}^{p}x_{ij}\beta_j)^2 + \lambda\sum_{j = 1}^{p}\rvert\beta_j\rvert\} \ \ \ (1)
$$
Which is the same as:
$$
\hat{\beta^{Lasso}} = arg\min_\beta\{\sum_{i = 1}^{N}(y_i - \beta_0 - \sum_{j = 1}^{p}x_{ij}\beta_j)^2\}\\
\text{subject to} \sum_{j = 1}^{p}\rvert\beta_j\rvert \leq t
$$
# The penalty term


# How does the shrinkage work
Now suppose that we have a dataset with 2 features. We first train a liner model for the data, and suppose $\beta_1$ and $\beta_2$ are the coefficients for the two features. Thus by the definition of the ridge regression constraint, we have:
$$
\beta_1^2 + \beta_2^2 \leq t
$$
Similarly, for the Lasso regression contraint, we have:
$$
\rvert\beta_1\rvert + \rvert\beta_2\rvert \leq t
$$
Thus, if we plot the two constraints in a 2-dimensional coordinate system, we have:
![](2dimConstraint.png)


# How does the shrinkage work in a Matrix perspective


# Generalization of shrinkage methods from Bayes perspective